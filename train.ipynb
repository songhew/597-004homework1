{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import argparse\n", "import time\n", "import os\n", "import torch\n", "import torch.nn as nn\n", "from torchvision import transforms\n", "from data_loader import get_loader\n", "from build_vocab import Vocabulary\n", "from models import DummyImageEncoder, DummyCaptionEncoder, CommonSpace\n", "from utils import create_exp_dir, Ranker"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Device configuration"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Paths to data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["IMAGE_ROOT = 'data/resized_images/'\n", "CAPT = 'data/captions/cap.{}.{}.json'\n", "DICT = 'data/captions/dict.{}.json'\n", "SPLIT = 'data/image_splits/split.{}.{}.json'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Loss function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["triplet_avg = nn.TripletMarginLoss(reduction='elementwise_mean', margin=1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def eval_batch(data_loader, image_encoder, caption_encoder, ranker):\n", "    ranker.update_emb(image_encoder)\n", "    rankings = []\n", "    loss = []\n", "    for i, (target_images, candidate_images, captions, lengths, meta_info) in enumerate(data_loader):\n", "        with torch.no_grad():\n", "            target_images = target_images.to(device)\n", "            target_ft = image_encoder.forward(target_images)\n", "            candidate_images = candidate_images.to(device)\n", "            candidate_ft = image_encoder.forward(candidate_images)\n", "            captions = captions.to(device)\n", "            caption_ft = caption_encoder(captions, lengths)\n", "            target_asins = [ meta_info[m]['target'] for m in range(len(meta_info)) ]\n", "            rankings.append(ranker.compute_rank(candidate_ft + caption_ft, target_asins))\n", "            m = target_images.size(0)\n", "            random_index = [m - 1 - n for n in range(m)]\n", "            random_index = torch.LongTensor(random_index)\n", "            negative_ft = target_ft[random_index]\n", "            loss.append(triplet_avg(anchor=(candidate_ft + caption_ft),\n", "                               positive=target_ft, negative=negative_ft))\n", "    metrics = {}\n", "    rankings = torch.cat(rankings, dim=0)\n", "    metrics['score'] = 1 - rankings.mean().item() / ranker.data_emb.size(0)\n", "    metrics['loss'] = torch.stack(loss, dim=0).mean().item()\n", "    return metrics"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train(args):\n", "    # Image preprocessing, normalization for the pretrained resnet\n", "    transform = transforms.Compose([ \n", "        transforms.RandomCrop(args.crop_size),\n", "        transforms.RandomHorizontalFlip(),\n", "        transforms.ToTensor(), \n", "        transforms.Normalize((0.485, 0.456, 0.406), \n", "                             (0.229, 0.224, 0.225))])\n", "    transform_dev = transforms.Compose([\n", "        transforms.CenterCrop(args.crop_size),\n", "        transforms.ToTensor(),\n", "        transforms.Normalize((0.485, 0.456, 0.406),\n", "                             (0.229, 0.224, 0.225))])\n", "    vocab = Vocabulary()\n", "    vocab.load(DICT.format(args.data_set))\n\n", "    # Build data loader\n", "    data_loader = get_loader(IMAGE_ROOT.format(args.data_set),\n", "                             CAPT.format(args.data_set, 'train'),\n", "                             vocab, transform,\n", "                             args.batch_size, shuffle=True, return_target=True, num_workers=args.num_workers)\n", "    data_loader_dev = get_loader(IMAGE_ROOT.format(args.data_set),\n", "                                 CAPT.format(args.data_set, 'val'),\n", "                                 vocab, transform_dev,\n", "                                 args.batch_size, shuffle=False, return_target=True, num_workers=args.num_workers)\n", "    ranker = Ranker(root=IMAGE_ROOT.format(args.data_set), image_split_file=SPLIT.format(args.data_set, 'val'),\n", "                    transform=transform_dev, num_workers=args.num_workers)\n", "    save_folder = '{}/{}-{}'.format(args.save, args.data_set, time.strftime(\"%Y%m%d-%H%M%S\"))\n", "    create_exp_dir(save_folder, scripts_to_save=['models.py', 'data_loader.py', 'train.py', 'build_vocab.py', 'utils.py'])\n", "    def logging(s, print_=True, log_=True):\n", "        if print_:\n", "            print(s)\n", "        if log_:\n", "            with open(os.path.join(save_folder, 'log.txt'), 'a+') as f_log:\n", "                f_log.write(s + '\\n')\n", "    logging(str(args))\n", "    # Build the dummy models\n", "    common_space_embedding = CommonSpace(embed_size=args.embed_size)\n", "    image_encoder = DummyImageEncoder(args.embed_size, common_space_embedding).to(device)\n", "    caption_encoder = DummyCaptionEncoder(vocab_size=len(vocab), vocab_embed_size=args.embed_size * 2,\n", "                                          embed_size=args.embed_size, common_space_embedding=common_space_embedding).to(device)\n", "    image_encoder.train()\n", "    caption_encoder.train()\n", "    params = image_encoder.get_trainable_parameters() + caption_encoder.get_trainable_parameters()\n", "    current_lr = args.learning_rate\n", "    optimizer = torch.optim.Adam(params, lr=current_lr)\n", "    cur_patient = 0\n", "    best_score = float('-inf')\n", "    stop_train = False\n", "    total_step = len(data_loader)\n", "    # epoch = 1 for dummy setting\n", "    for epoch in range(5):\n", "        for i, (target_images, candidate_images, captions, lengths, meta_info) in enumerate(data_loader):\n", "            target_images = target_images.to(device)\n", "            target_ft = image_encoder.forward(target_images)\n", "            candidate_images = candidate_images.to(device)\n", "            candidate_ft = image_encoder.forward(candidate_images)\n", "            captions = captions.to(device)\n", "            caption_ft = caption_encoder(captions, lengths)\n\n", "            # random select negative examples\n", "            m = target_images.size(0)\n", "            random_index = [m - 1 - n for n in range(m)]\n", "            random_index = torch.LongTensor(random_index)\n", "            negative_ft = target_ft[random_index]\n", "            loss = triplet_avg(anchor=(candidate_ft + caption_ft),\n", "                               positive=target_ft, negative=negative_ft)\n", "            caption_encoder.zero_grad()\n", "            image_encoder.zero_grad()\n", "            loss.backward()\n", "            optimizer.step()\n", "            if i % args.log_step == 0:\n", "                logging(\n", "                    '| epoch {:3d} | step {:6d}/{:6d} | lr {:06.6f} | train loss {:8.3f}'.format(epoch, i, total_step,\n", "                                                                                                 current_lr,\n", "                                                                                                 loss.item()))\n", "        image_encoder.eval()\n", "        caption_encoder.eval()\n", "        logging('-' * 77)\n", "        metrics = eval_batch(data_loader_dev, image_encoder, caption_encoder, ranker)\n", "        logging('| eval loss: {:8.3f} | score {:8.5f} / {:8.5f} '.format(\n", "            metrics['loss'], metrics['score'], best_score))\n", "        logging('-' * 77)\n", "        image_encoder.train()\n", "        caption_encoder.train()\n", "        dev_score = metrics['score']\n", "        if dev_score > best_score:\n", "            best_score = dev_score\n", "            # save best model\n", "            resnet = image_encoder.delete_resnet()\n", "            torch.save(image_encoder.state_dict(), os.path.join(\n", "                save_folder,\n", "                'image-{}.th'.format(args.embed_size)))\n", "            image_encoder.load_resnet(resnet)\n", "            torch.save(caption_encoder.state_dict(), os.path.join(\n", "                save_folder,\n", "                'cap-{}.th'.format(args.embed_size)))\n", "            cur_patient = 0\n", "        else:\n", "            cur_patient += 1\n", "            if cur_patient >= args.patient:\n", "                current_lr /= 2\n", "                for param_group in optimizer.param_groups:\n", "                    param_group['lr'] = current_lr\n", "                if current_lr < args.learning_rate * 1e-3:\n", "                    stop_train = True\n", "                    break\n", "        if stop_train:\n", "            break\n", "    logging('best_dev_score: {}'.format(best_score))"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}